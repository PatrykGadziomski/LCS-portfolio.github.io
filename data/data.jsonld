{
  "@context": {
    "schema": "https://schema.org/",
    "portfolio": "https://patrykgadziomski.github.io/lcs-portfolio.github.io#",
    "relatedTo": "schema:relatedLink",
    "hasPart": "schema:hasPart"
  },
  "@graph": [
    {
      "@id": "portfolio:intro",
      "@type": "schema:Article",
      "schema:name": "Intro",
      "schema:authors": {
        "@id": "portfolio:patryk"
      },
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:content": "<h2>Wilkommen auf meinem Lernportfolio!</h2><p>Dieses Portfolio dient der Dokumentation meiner Lernreise im Modul Schnittstellen und Datenformate im Studiengang Biliotheksinformatik an der Technischen Hochschule Wildau. Hier sammle ich Notizen und Erkenntnisse zu verschiedenen Themen aus dem Modul.</p><br><p>Das Besondere an diesem Lernportfolio ist die Integration eines Knowledge Graphs auf der rechten Seite. Der Graph visualisiert die Struktur und Beziehungen zwischen den verschiedenen Notizen, ähnlich wie in Obsidian (Keine Werbung! Aber eine sehr tolle Software ;)).</p><br><p>Jeder Knoten repräsentiert eine Notiz, und die Verbindungen zeigen, wie die Themen miteinander zusammenhängen. Klicke auf einen Knoten im Graph, um direkt zum entsprechenden Artikel zu springen!</p><br><p>Der Aufbau dieses Lerportfolios ist in zeitliche Abschnitte aufgeteilt, je nach den Terminen, an dennen die Vorlesungen stattfanden. In diesen wird kurz zusammengefasst was an diesen Tagen stattfand und was ich gelernt habe.</p><br><p>Bei der technischen Umsetzung dieses Lernportfolios wurden Claude (ANTHROPIC) und Euria (INFOMANIAK) eingesetzt um die Grundlegenden FUnktionalitäten zu generieren. Weitere Funktionen, Strukturierung und Datenkuration wurde durch den Entwickler durchgeführt. Der Text in idesem Portfolio wurde ohne Hilfe der Sprachmodelle erstellt.</p><br><p>Folgende Fragen werden am Ende noch man als zussamfassendes Fazi beantortet:</p><br><ul style='padding-left: 15px;'><li>Was haben Sie gelernt (inhaltlich, persönlich, sonstiges)?</il><li>Was war ihr größter Lerngewinn?</li><li>Was war besonders überraschend? Was hat Sie irritiert?</li><li>Wo sehen Sie für sich noch Bedarf an Weiterbildung zum Thema Datenformate? Was nehmen Sie sich für die nächste Zeit vor?</li></ul>"
    },
    {
      "@id": "portfolio:vorlesung-1",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 1",
      "schema:startDate": "2025-10-15",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:references": [
        {"@id": "portfolio:datenformate-gbv"},
        {"@id": "portfolio:handbuch-it-in-bibliotheken"},
        {"@id": "portfolio:understanding-metadata"},
        {"@id": "portfolio:kleines-handbuch-metadaten"},
        {"@id": "portfolio:zugang-gestalten"},
        {"@id": "portfolio:date-meme"},
        {"@id": "portfolio:einführung-in-skos"},
        {"@id": "portfolio:marc-for-bibliographic"},
        {"@id": "portfolio:marc-dublin-core"},
        {"@id": "portfolio:marc-21"},
        {"@id": "portfolio:einführung-pica"}
      ],
      "schema:content": "<p>Dies war der dritte Tag des Studiums der Bibliotheksinformatik an der TH Wildau, zugleich der erste Tag des Moduls „Schnittstellen und Datenformate”. Das Modul ist in zwei Teile aufgeteilt, wobei der erste Teil Datenformate behandelt, die von Tracy Arndt gelehrt werden.</p><br><p>Die Vorlesung begann mit den üblichen organisatorischen Inhalten und der Vorstellung. Anschließend wurden die Ziele des Teilmoduls sowie die einzelnen Blöcke vorgestellt. Der erste Block (Tag 1 und 2) befasst sich mit Terminologie, Heterogenität, Interoperabilität und Organisation. Natürlich durften auch die Prüfungsleistungen und die dazugehörigen Bewertungsdimensionen nicht fehlen. Das Lernportfolio als Prüfungsleistung finde ich persönlich sehr gut. Bereits im Bachelorstudium hatte ich oft diese Form der Prüfungsleistung. Das Schöne daran ist für mich die Freiheit, die man den Studierenden lässt, ihr Portfolio auf ihre ganz eigene Art zu erstellen. Außerdem bin ich der festen Überzeugung, dass man nur durch Reflexion des Gelernten merkt, ob man die Inhalte eines Moduls/eines Fachs verstanden hat. Manchmal führe ich selbst ein Lerntagebuch für Themen, die ich mir selbst beibringe, da ich mir dabei die Frage stellen muss, was ich überhaupt gelernt habe.</p><br><p>Zunächst klärten wir, was Daten überhaupt sind, also die Terminologie. Das war für mich nichts Neues, denn ich hatte in meinem Bachelor einen Schwerpunkt auf Daten gesetzt. Was Metadaten sind, war mir auch dadurch bereits klar. Bibliothekarische Metadaten waren mir jedoch nur zum Teil ein Begriff. Ich hatte zwar PICA und MARC im Studium, aber weder Bibframe noch Linked-Data-Formate. Linked Data finde ich äußerst interessant und möchte es überall dort verwenden, wo es möglich ist, um es wirklich zu verstehen und zu erfahren, wie es funktioniert. Wie sich zeigt, bekomme ich dazu auch die Gelegenheit (siehe Tag 4).</p><br><p>Im Anschluss daran folgte eine Gruppenarbeit, in der wir uns mit verschiedenen Begriffen auseinandersetzen sollten. Dazu wurden uns Quellen zur Verfügung gestellt. Meine Gruppe und ich entwarfen daraufhin eine Visualisierung. Die Ergebnisse wurden anschließend präsentiert. Es wurde rege darüber diskutiert, was genau das Datenmodell ist und ob bzw. wie überall Standards auftreten können. Es war interessant zu sehen, dass selbst Menschen, die täglich mit dieser Thematik arbeiten, Probleme damit haben, alles genau zu beschreiben. Das war für mich zum Teil ein „Aha“- und ein „Wow“-Moment. Früher dachte ich immer, dass man, sobald man in einem Bereich arbeitet, diesen komplett überblickt und versteht. Mittlerweile weiß ich es besser und auch, dass es eine ständige Lernreise ist. In der Gruppe haben wir viel diskutiert und die Abbildung komplett neu angefertigt.</p><br><img src='imgs/datenformate.svg' style='width: 100%;'><br><p>Spannend fand ich die Interoperabilität von Daten. Damit habe ich leider nicht viele Berührungspunkte, aber das möchte ich gerne ändern. Ich fand die Konvertierung zwischen Formaten schon immer spannend. Ein Skript zu schreiben, das ein Format in ein anderes konvertieren kann, hat etwas Beruhigendes. Dabei merke ich, dass ich statt vorhandener Software lieber eigene Skripte schreibe, was Vor- und Nachteile hat.</p><br><p>Vor allem das Thema, dass es so viele Standards gibt und man sich auf keinen einigen kann (aus mehreren Gründen), fand ich sehr spannend. Im Modul „Künstliche Intelligenz” bin ich dabei, einen Linked-Data-Datensatz aufzubauen (dazu später mehr) und ich hätte fast einen eigenen Standard entwickelt, weil ich dachte: „Ah, wieso nicht? Dann kann ich es genau an meine Wünsche anpassen!“ Am Ende habe ich mich doch für Standards entschieden, da sie alles beinhalteten, was ich brauchte; ich musste sie mir nur genauer anschauen.</p><br><p>In meinem Alltag kann ich durchaus behaupten, dass ich viel mit Daten zu tun habe. Das sollte jeder behaupten können, da wir von Daten umgeben sind. Im beruflichen Kontext habe ich nur mit ISBD und MARC zu tun, da ich in meiner Bibliothek die Katalogisierung mit Koha durchführe. In meiner anderen Tätigkeit bin ich mit Webentwicklung beschäftigt, weshalb ich auch mit Datenformaten umgehen können muss. Das war mir bereits aus dem Bachelorstudium bekannt. Dieser erste Tag war für mich somit eine nette Auffrischung, aber ich habe nicht viel Neues gelernt.</p>"
    },
    {
      "@id": "portfolio:vorlesung-2",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 2",
      "schema:startDate": "2025-10-17",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-1"
      },
      "schema:references": [
        {"@id": "portfolio:einführung-pica"},
        {"@id": "portfolio:processing-marc-21"},
        {"@id": "portfolio:marc-must-die"},
        {"@id": "portfolio:marc-history-implications"},
        {"@id": "portfolio:handbuch-it-in-bibliotheken"},
        {"@id": "portfolio:library-reference-model"},
        {"@id": "portfolio:link-data-in-libraries"},
        {"@id": "portfolio:semantic-web"}
      ],
      "schema:content": "<p>An diesem Tag gingen wir tiefer in die Materie der Metadaten ein und begannen mit der Geschichte von PICA und MARC. Das Thema fand ich persönlich nicht besonders interessant, aber es war dennoch spannend zu sehen, wann diese entstanden sind und wie sich die Anforderungen an Metadatenformate gewandelt haben. Ich merke, dass mein Interesse eher der Verwendung der Daten bzw. Datenformate gilt als ihrer Geschichte.</p><p>Das größte Thema dieses Tages war die Metadatenorganisation. Ein hochspannendes Thema, das, finde ich, sehr kompliziert werden kann. Allein die Tatsache, dass es Austauschformate geben muss, da es so viele unterschiedliche Metadatenformate gibt, macht das Thema kompliziert. Ich habe mich gefragt, wieso es nicht ein Datenformat/Metadatenformat für alles geben kann. Aber das ist klar: Man kann nie alles abbilden. Selbst wenn man ein solches Format immer weiter erweitern würde, würde es so viele Felder beinhalten, dass es allein aus Speichergründen nicht benutzt werden könnte. Hier geht es vielmehr um die aufgabenspezifische Benutzung. Es ist zwar eine schöne Vorstellung, ein „Omni“-Datenformat/Metadatenformat zu entwickeln, aber auch eine unmögliche.</p><br><p>Was man hierbei wunderbar sehen kann: Metadaten gibt es überall. Egal, in welchem Bereich man arbeitet, kommt man immer mit ihnen in Kontakt, auch wenn man es vielleicht nicht weiß. Für mich wurde auch noch einmal klar, wo überall Metadaten vorkommen. Überall! In jedem Datenfluss kommen Daten und Metadaten vor. Ich habe mir in diesem Zusammenhang meine alten Arbeiten aus dem Bachelorstudium noch einmal angeschaut und gemerkt, an wie vielen Stellen das eigentlich der Fall war, ich aber nicht so sehr darauf eingegangen bin (oder gar nicht). Jetzt hätte ich nur gerne die Zeit, alles noch einmal durchzugehen und abzuändern.</p><br><p>Das FRBR-Modell war mir bereits aus dem Bachelor bekannt. Ich wusste jedoch nicht, dass es mehrere Varianten gibt bzw. dass es weiterentwickelt wurde. Das FRBR-LRM hat mich sehr überrascht. Da es sich aber um ein sehr theoretisches Modell handelt, entsteht für mich persönlich wieder das gleiche Problem, dass sich damit nicht alles abbilden lässt und man sich ewig darüber streiten kann.</p><br><p>Danach kam das unglaublich tolle Thema „Linked (Open) Data”. Ich hatte im Rahmen meines Studiums bereits damit zu tun, da sich mein Professor damit beschäftigte, aber ich konnte mir darunter nie etwas vorstellen. Für mich war es wieder eine sehr theoretische Vorstellung. Das Web, in dem Daten als Knoten definiert sind, die wiederum Verbindungen zu anderen Knoten und somit zu anderen Daten darstellen. Theoretisch war es mir klar, aber ich konnte mir beim besten Willen nicht vorstellen, wie es in der Praxis benutzt werden sollte (ohne zu wissen, dass ich es selbst seit Jahren benutze ...). Aber endlich habe ich es verstanden! Es ist ein Knowledge Graph! Ein Knowledge Graph entsteht also durch das Verbinden von Daten (Nodes). Das ist dann Linked Data! Ich finde die Idee, dass das Web im Web-3.0-Modell komplett aus Daten besteht, die aufeinander verweisen und somit verbunden sind, unglaublich schön. Allein die Möglichkeit, semantische Anfragen zu stellen, um genau die Daten zu finden, die man braucht, ist für die Suche und somit für Suchmaschinen enorm wichtig. Leider wurde das durch das Aufkommen der großen Sprachmodelle verhindert. bzw. gebremst. Ich stelle mir ein perfektes Modell vielmehr als eine Verbindung von beiden Dingen vor. Das Semantic Web könnte eine unglaubliche Daten- und Informationsquelle darstellen, auf die KI-Agenten zugreifen könnten. Ich habe dazu einmal eine Abbildung gesehen, finde diese aber leider nicht mehr. Ich habe es mit ChatGPT und DALL-E versucht zu visualisieren.</p><br><img src='imgs/web3andAI.png' style='width: 100%;'><br><p>Jetzt: Warum denke ich, dass ich Linked Data bereits seit Jahren benutze?</p><br><p>Ich benutze seit Jahren Obsidian, um meine Notizen zu erstellen und zu verwalten. Die Software erlaubt es, Verbindungen zwischen Notizen zu erstellen. Somit war mir dieses Prinzip, Informationen zu verbinden, bereits sehr gut bekannt. Aber endlich habe ich es vollständig verstanden und die einzelnen Knoten in meinem Kopf zum Thema Linked Data haben sich endlich „verbunden” (XD). Ich habe meine Notizen die ganze Zeit wie Linked Data behandelt, ohne es zu wissen. Seitdem benutze ich Standards wie Schema, Dublin Core und Friend of a Friend, um meine Daten noch besser zu strukturieren und zu verknüpfen. Wieso? Im Modul „Künstliche Intelligenz” bearbeite ich mit einer Kommilitonin ein Projekt zum Thema „RAG”. Dafür erstelle ich, wie bereits gesagt, einen Linked-Data-Datensatz. Die Idee ist es, am Ende ein RAG-System zu haben, mit dem man im Obsidian Vault suchen bzw. dem RAG-System Fragen stellen kann. Sehr interessant war auch die Tatsache, dass Linked Data besser für das Training oder die Benutzung von KI-Modellen wie LLMs geeignet ist. Das passt sehr gut zu unserem Projekt. Wenn es für das Projekt gut funktioniert, werde ich es für meinen privaten Vault nutzen und kann somit mit „mir selbst reden”, da dieser viele Informationen aus meinem Leben beinhaltet. (Der Datenschutz muss allerdings noch beachtet werden, denn ich möchte nicht, dass meine Daten am Ende in irgendein LLM geworfen werden.)</p>"
    },
    {
      "@id": "portfolio:vorlesung-3",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 3",
      "schema:startDate": "2025-12-03",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-2"
      },
      "schema:references": [
        {"@id": "portfolio:semantic-web"},
        {"@id": "portfolio:w3c-standards-and-drafts"},
        {"@id": "portfolio:semantic-web-working-ontologist"},
        {"@id": "portfolio:validating-rdf-data"}
      ],
      "schema:content": "<p>Das Thema „Linked Data” bzw. „Semantic Web” wurde an diesem Tag weiter vertieft. Und man kann durchaus behaupten, dass meine Lernkurve in diesem Bereich exponentiell war.</p><br><p>An diesem Tag habe ich endlich verstanden, wie man Ontologien aufbaut und RDF-Standards benutzt. (Zwar habe ich damit zuvor bereits etwas experimentiert, doch durch die Übung, die wir während der Vorlesung gemacht haben, wurde alles viel klarer.) Bisher dachte ich immer, dass Ontologien sehr komplexe, textuelle Bäume sind. Das trifft so gesehen zu, aber auch wieder nicht.</p><br><p>PS: Vor dem Master habe ich mir Jobs im Bereich der Ontologien angeschaut und dachte, dass sie zwar interessant klingen, ich es aber bestimmt nicht hinbekomme, weil mir das Wissen fehlt. Jetzt weiß ich: Mir hat das Wissen gefehlt, aber die Praxis war da. Ich wusste nur nicht, dass das, was ich in Obsidian erstellt habe, Ontologien waren.</p><br><p>Ontologien lassen sich als das darstellen, was ich sowieso fast jeden Tag gemacht habe bzw. für mein privates Leben viel tue. Man erstellt Beschreibungen von Objekten und deren Beziehungen zueinander in Form eines Knowledge Graphs. Das war wie eine Erleuchtung für mich. Anhand dieser Ontologien kann man Objekte beschreiben. Dafür gibt es verschiedene Standards. Das war ein großer Lerneffekt und ein noch größerer „Aha“-Moment. Vor allem durch das Üben mit WebVOWL wurde mir klar, wie Ontologien aussehen können.</p><br><p>Zwei Übungen wurden durchgeführt, welche mir unglaublich viel gebrach haben: <a href='data/PG_Ontology.ttl'>Eigene Ontologie</a> und <a href='data/bim25graph.ttl'>BIM25 Ontologie</a>.</p><br><p>Neben diesem Thema wurde SPARQL kurz angesprochen, das aber am nächsten Tag genauer behandelt wird.</p><br><p>Sehr schöne Ressourcen die genannt wurden will ich hier nochmal verewigen: <ul style='padding-left: 15px;'><li><a href='https://www.ldf.fi/service/rdf-grapher' target='_blank'>RDF Grapher</a></li><li><a href='https://issemantic.net/rdf-visualizer' target='_blank'>Visualize RDF graph linked data as a connected diagram</a></li><li><a href='https://issemantic.net/rdf-converter' target='_blank'>Online RDF converter and validator to JSON-LD, Microdata, Turtle, TriG, RDF-star or any other serialization format</a></li><li><a href='http://ttl.summerofcode.be/' target='_blank'>IDLab Turtle Validator</a></li></ul></p>"
    },
    {
      "@id": "portfolio:vorlesung-4",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 4",
      "schema:startDate": "2025-12-04",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-3"
      },
      "schema:references": [
        {"@id": "portfolio:sparql-overview"},
        {"@id": "portfolio:wikidata-sparql-tutorial"},
        {"@id": "portfolio:wikidata-sparql-query-service"},
        {"@id": "portfolio:sparql-by-example"},
        {"@id": "portfolio:wikibooks-sparql"},
        {"@id": "portfolio:sparql-tutorial"},
        {"@id": "portfolio:learning-sparql"}
      ],
      "schema:content": "<p>An diesem Tag wurde SPARQL vorgestellt. Ich muss zugeben, dass es der Tag war, an dem ich am wenigsten mitgenommen habe. Nicht, weil das Thema nicht spannend ist. Ich finde es unglaublich, was man mit SPARQL alles abfragen kann. Aber ich bin einfach nicht für Online-Vorlesungen gemacht. In einer normalen Vorlesung höre ich zu, achte auf und stelle gelegentlich Fragen. Dabei kann ich mich komplett darauf einstellen. Bei einer Online-Vorlesung funktioniert das nicht, meine Konzentration schweift komplett ab. Aus diesem Grund fiel es mir schwer, der Vorlesung zu folgen, aber ein paar Dinge konnte ich doch mitnehmen.</p><br><p>Mit SPARQL hatte ich bisher nur einen Berührungspunkt: Im Bachelor-Modul „Metadatenmanagement” habe ich es kennengelernt, aber ich muss gestehen, dass ich es bis heute nicht ganz durchblicke. Mir ist durchaus verständlich, wie SPARQL funktioniert, aber die Syntax ist mein echtes Problem. Ich müsste mir theoretisch alle Codes für die Entitäten merken, wenn es keine Liste gibt bzw. ich keine zur Hand habe. Das ist mein größtes Problem. Das ist sehr ungewohnt. Wenn ich Python-Code oder eine SQL-Abfrage schreibe, muss ich normalerweise nichts nachschauen, außer wie die Spalten heißen. Ich weiß allerdings nicht, ob man das so vergleichen kann. Aus diesem Grund und aufgrund meiner mangelnden Online-Konzentration hatte ich Schwierigkeiten, den Übungsaufgaben zu folgen. Ich habe mir aber vorgenommen, das nachzuholen, denn ich sehe ein riesiges Potenzial für Information Retrieval und eventuell auch für meine privaten Projekte, vor allem, da man auch geografische Daten abfragen kann. Was mir tatsächlich leichtfiel, waren die Funktionen in SPARQL wie Filtern oder Gruppieren, da diese sehr ähnlich zu SQL sind.</p><br><p>Ich habe mir vorgenommen, das Buch Learning SPARQL: Querying and Updating with SPARQL von O’Reilly Media durchzuarbeiten, um am Ende behaupten zu können, SPARQL wirklich verstanden zu haben. Da ich aber derzeit kein Projekt habe, in dem ich SPARQL verwenden muss, und auch nicht die Zeit habe, das Buch durchzuarbeiten, wird sich das Thema SPARQL für mich leider noch etwas nach hinten verschieben."
    },
    {
      "@id": "portfolio:vorlesung-5",
      "@type": "schema:Event",
      "schema:name": "Vorlesung 5",
      "schema:startDate": "2026-01-16",
      "schema:about": {
        "@id": "portfolio:modul"
      },
      "schema:organizer": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:previousItem": {
        "@id": "portfolio:vorlesung-4"
      },
      "schema:references": [
        {"@id": "portfolio:openrefine-user-manual"},
        {"@id": "portfolio:metafacture-tutorial"}
      ],
      "schema:content": "<p>Unverzichtbar sind Werkzeuge, mit denen sich Daten ansehen, bearbeiten, analysieren und verarbeiten lassen. Dabei wurden unterschiedliche Softwares gezeigt und zwei davon haben wir in einem kleinen Workshop selbst ausprobiert.</p><br><p>Am letzten Tag des Teilmoduls fand ein Workshop zu Open Refine und Metafacture statt. Bei Open Refine wurden verschiedene Funktionen anhand der Dokumentation gezeigt und ausprobiert. Bei Metafacture wurden die Funktionen anhand eines Tutorials gezeigt. Persönlich nutze ich solche Software in meiner Praxis nicht wirklich. Wenn ich Operationen zum Filtern oder Clustering großer Datenmengen durchführe, dann mache ich das meistens in Python mit Bibliotheken, die mit großen Datenmengen arbeiten können. Da ich das oft tue, bin ich mit Python bei der Datenanalyse auch schneller als mit Software mit einer GUI. Ich kann aber nachvollziehen, dass eine GUI-Software für Personen, die nicht oft programmieren, von Vorteil ist. Sehr praktisch fand ich den Abgleich der Daten mit anderen Datensätzen (z. B. dem Datensatz der DNB).</p><br><p>Bei Metafacture war mein Eindruck ähnlich. Ich fand das Einlesen eines Datensatzes (z. B. in JSON) und das Exportieren in einem anderen Datenformat aber sehr praktisch. Ansonsten konnte ich für meine alltägliche Arbeit keinen großen Nutzen daraus ziehen. Das kann sich aber noch ändern.</p><br><p>Am Ende folgte eine kleine Diskussion, die mehr oder weniger genau in diese Richtung ging. Einerseits hat man das Potenzial der Software gesehen, aber da sie nur im Nischenbereich von sehr wenigen Leuten eingesetzt wird, ist es fragwürdig, wieso man sie überhaupt lernen sollte. Vor allem, da man sich erst mit der Software auseinandersetzen müsste, da sie nicht ganz unkomplex ist. Ich glaube, die in der Gruppe, die täglich programmieren, sind es eher gewohnt, solche Operationen mit Python und Pandas durchzuführen (solange der Speicher es erlaubt), während die anderen das Problem haben, dass sie in ihrer alltäglichen Arbeit einfach keine Verwendung für solche Daten-Software finden, da sie nicht täglich mit Daten arbeiten."
    },
    {
      "@id": "portfolio:aufgabe-1",
      "@type": "schema:Assignment",
      "schema:name": "Aufgabe 1 - Datenworkflow",
      "schema:educationalLevel": "Master",
      "schema:courseCode": {
        "@id": "portfolio:modul"
      },
      "schema:content": "<h3>Die Staatsbibliothek zu Berlin hat auf ihrer Webseite den <a href='https://digital.staatsbibliothek-berlin.de/ueber-digitalisierte-sammlungen/digiworkflow'>Workflow für die Digitalisierung</a> veröffentlicht.<br>Erstellen sie aus der Liste ein Diagramm das den Workflow grafisch darstellt. Reflektieren sie anschließend den Workflow und beschreiben sie die eingesetzten Datenformate und Schnittstellen (TIFF, METS/MODS, IIIF-Manifest, OAI-PMH)</h3><br><img src='imgs/digitalisierte_sammlungen.svg' style='width: 100%;'>"
    },
    {
      "@id": "portfolio:aufgabe-2",
      "@type": "schema:Assignment",
      "schema:name": "Aufgabe 2 - Linked Data",
      "schema:educationalLevel": "Master",
      "schema:courseCode": {
        "@id": "portfolio:modul"
      },
      "schema:references": [
        {"@id": "portfolio:linked-data"}
      ],
      "schema:content": "<h3>Bitte lesen Sie den Text: <a href='https://www.w3.org/DesignIssues/LinkedData.html' target='_blank'>Linked Data</a>. Fassen sie mit ihren eigenen Worten zusammen was Linked Data ausmacht.</h3><br><p>Linked Data beschreibt das Konzept zur Vernetzung von Daten im Web, die sowohl von Menschen als auch von Maschinen lesbar sind und die es ermöglichen, durch gezielt gesetzte Links Informationen zu entdecken.</p><br><p>Es gibt vier Aspekte, die Linked Data ausmachen (nach Tim Berners-Lee): Alle Objekte werden durch einen eindeutigen Namen (URI) gekennzeichnet; Es werden HTTP-URIs verwendet, damit Menschen und Maschinen diese Objekte im Web aufrufen können. Beim Aufruf werden die Informationen hinter dem URI aufgerufen (Dereferenzierung); Bei einem Aufruf sollen zusätzliche Informationen durch RDF oder SPARQL bereitgestellt werden; Die Objekte beinhalten Links zu anderen verwandten Objekten.</p><br><p>Durch das Verbinden der Objekte nach RDF entsteht ein durchsuchbarer Graph, in dem man verschiedene (auch unerwartete) Objekte wiederfinden kann, die Verbindungen zum gesuchten Objekt haben. Tim Berners-Lee ergänzt das Konzept um ein 5-Sterne-Schema für Linked (Open) Data: offene Lizenz, maschinenlesbar, offene Datenformate, RDF-Standards und Verlinkung. Wichtig: Linked Data kann intern und privat genutzt werden, aber es braucht eine offene Lizenz, damit es zu Linked Open Data wird.</p>"
    },
    {
      "@id": "portfolio:patryk",
      "@type": "schema:Person",
      "schema:name": "Patryk Gadziomski",
      "schema:affiliation": {
        "@id": "portfolio:th-wildau"
      },
      "schema:memberOf": {
        "@id": "portfolio:studiengang"
      }
    },
    {
      "@id": "portfolio:tracy-arndt",
      "@type": "schema:Person",
      "schema:name": "Tracy Arndt",
      "schema:jobTitle": "Dozentin",
      "schema:affiliation": {
        "@id": "portfolio:th-wildau"
      }
    },
    {
      "@id": "portfolio:th-wildau",
      "@type": "schema:EducationalOrganization",
      "schema:name": "Technische Hochschule Wildau",
      "hasPart": {
        "@id": "portfolio:wit"
      }
    },
    {
      "@id": "portfolio:wit",
      "@type": "schema:EducationalOrganization",
      "schema:name": "Wildau Institue of Technology",
      "schema:isPartOf": [
        {"@id": "portfolio:th-wildau"}
      ]
    },
    {
      "@id": "portfolio:studiengang",
      "@type": "schema:EducationalOccupationalProgram",
      "schema:name": "Bibliotheksinformatik",
      "schema:provider": [
        {"@id": "portfolio:th-wildau"},
        {"@id": "portfolio:wit"}
      ],
      "hasPart": {
        "@id": "portfolio:modul"
      }
    },
    {
      "@id": "portfolio:modul",
      "@type": "schema:Course",
      "schema:name": "Schnittstellen und Datenformate",
      "schema:isPartOf": [
        {"@id": "portfolio:studiengang"}
      ],
      "schema:teacher": {
        "@id": "portfolio:tracy-arndt"
      },
      "schema:student": {
        "@id": "portfolio:patryk"
      }
    },
    {
      "@id": "portfolio:datenformate-gbv",
      "@type": "schema:CreativeWork",
      "schema:name": "Datenformate GBV",
      "schema:url": "https://format.gbv.de/",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:handbuch-it-in-bibliotheken",
      "@type": "schema:CreativeWork",
      "schema:name": "Handbuch IT in Bibliotheken",
      "schema:url": "https://it-in-bibliotheken.de/contributors.html",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:understanding-metadata",
      "@type": "schema:CreativeWork",
      "schema:name": "Understanding Metadata: What is Metadata, and What is it For?",
      "schema:url": "https://www.niso.org/publications/understanding-metadata-2017",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:kleines-handbuch-metadaten",
      "@type": "schema:CreativeWork",
      "schema:name": "Kleines Handbuch Metadaten",
      "schema:url": "https://www.yumpu.com/de/document/view/23832049/kleines-handbuch-metadaten",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:zugang-gestalten",
      "@type": "schema:CreativeWork",
      "schema:name": "Zugang gestalten - Eine Anleitung für schlechte Standards",
      "schema:url": "https://www.youtube.com/watch?v=o51FOLsh4Ec",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:date-meme",
      "@type": "schema:CreativeWork",
      "schema:name": "Date Meme",
      "schema:url": "https://github.com/SamAmco/track-and-graph/issues/197#issuecomment-1445226139",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:einführung-in-skos",
      "@type": "schema:CreativeWork",
      "schema:name": "Einführung in SKOS am Beispiel von Open Educational Resources (OER)",
      "schema:url": "https://dini-ag-kim.github.io/skos-einfuehrung/#/",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:marc-for-bibliographic",
      "@type": "schema:CreativeWork",
      "schema:name": "Marc 21 Format for bibliographic data",
      "schema:url": "https://www.loc.gov/marc/bibliographic/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-dublin-core",
      "@type": "schema:CreativeWork",
      "schema:name": "Marc to Dublin Core Crosswalk",
      "schema:url": "https://www.loc.gov/marc/marc2dc.html",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-21",
      "@type": "schema:CreativeWork",
      "schema:name": "Marc 21",
      "schema:url": "https://www.dnb.de/DE/Professionell/Metadatendienste/Exportformate/MARC21/marc21_node.html",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:einführung-pica",
      "@type": "schema:CreativeWork",
      "schema:name": "EInführung in die Verarbeitung von PICA-Daten",
      "schema:url": "https://pro4bib.github.io/pica/#/",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:processing-marc-21",
      "@type": "schema:CreativeWork",
      "schema:name": "Processing MARC 21",
      "schema:url": "https://jorol.github.io/processing-marc/#/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-must-die",
      "@type": "schema:CreativeWork",
      "schema:name": "MARC Must Die",
      "schema:url": "https://www.libraryjournal.com/story/marc-must-die",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:marc-history-implications",
      "@type": "schema:CreativeWork",
      "schema:name": "MARC - Its history and implicaions",
      "schema:url": "https://scispace.com/pdf/marc-its-history-and-implications-3q74lbh06u.pdf",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:library-reference-model",
      "@type": "schema:CreativeWork",
      "schema:name": "IFLA Library Reference Model - A Conceptual Model for Bibliographic Information",
      "schema:url": "https://www.ifla.org/wp-content/uploads/2019/05/assets/cataloguing/frbr-lrm/ifla-lrm-august-2017.pdf",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:link-data-in-libraries",
      "@type": "schema:CreativeWork",
      "schema:name": "Linked Data in Libraries - A Case Study of Harvesting and Sharing Bibliographic Metadata with BIBFRAME",
      "schema:url": "https://www.researchgate.net/publication/276102000_Linked_Data_in_Libraries_A_Case_Study_of_Harvesting_and_Sharing_Bibliographic_Metadata_with_BIBFRAME",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:semantic-web",
      "@type": "schema:CreativeWork",
      "schema:name": "Semantic Web - Grundlagen",
      "schema:url": "https://link.springer.com/book/10.1007/978-3-540-33994-6",
      "schema:inLanguage": "de"
    },
    {
      "@id": "portfolio:w3c-standards-and-drafts",
      "@type": "schema:CreativeWork",
      "schema:name": "W3C standards and drafts",
      "schema:url": "https://www.w3.org/TR/?tags[0]=data",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:semantic-web-working-ontologist",
      "@type": "schema:CreativeWork",
      "schema:name": "Semantic Web for the Working Ontologist - Effective Modeling in RDFS and OWL",
      "schema:url": "https://www.sciencedirect.com/book/monograph/9780123859655/semantic-web-for-the-working-ontologist",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:validating-rdf-data",
      "@type": "schema:CreativeWork",
      "schema:name": "Validating RDF Data",
      "schema:url": "https://book.validatingrdf.com/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:sparql-overview",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL 1.1 Overview",
      "schema:url": "https://www.w3.org/TR/sparql11-overview/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:wikidata-sparql-tutorial",
      "@type": "schema:CreativeWork",
      "schema:name": "Wikidata: SPARQL Tutorial",
      "schema:url": "https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:wikidata-sparql-query-service",
      "@type": "schema:CreativeWork",
      "schema:name": "Wikidata:SPARQL query service/queries/examples",
      "schema:url": "https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:sparql-by-example",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL By Example: The Cheat Sheet",
      "schema:url": "https://www.iro.umontreal.ca/~lapalme/ift6281/sparql-1_1-cheat-sheet.pdf",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:wikibooks-sparql",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL",
      "schema:url": "https://en.wikibooks.org/wiki/SPARQL",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:sparql-tutorial",
      "@type": "schema:CreativeWork",
      "schema:name": "SPARQL Tutorial",
      "schema:url": "https://jena.apache.org/tutorials/sparql.html",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:learning-sparql",
      "@type": "schema:CreativeWork",
      "schema:name": "Learning SPARQL: Querying and Updating with SPARQL",
      "schema:url": "https://www.oreilly.com/library/view/learning-sparql-2nd/9781449371449/",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:openrefine-user-manual",
      "@type": "schema:CreativeWork",
      "schema:name": "OpenRefine user manual",
      "schema:url": "https://openrefine.org/docs",
      "schema:inLanguage": "en"
    },
    {
      "@id": "portfolio:metafacture-tutorial",
      "@type": "schema:CreativeWork",
      "schema:name": "Metafacture Tutorial",
      "schema:url": "https://metafacture.github.io/metafacture-tutorial/",
      "schema:inLanguage": "en"
    },
    {
    "@id": "portfolio:linked-data",
    "@type": "schema:CreativeWork",
    "schema:name": "Linked Data",
    "schema:url": "https://www.w3.org/DesignIssues/LinkedData.html",
    "schema:inLanguage": "en"
  }
  ]
}